{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://mle_20240325_54955bf804:6e3f607018b444f69359510efb12da90@rc1b-uh7kdmcx67eomesf.mdb.yandexcloud.net:6432/playground_mle_20240325_54955bf804\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "\n",
    "def create_connection():\n",
    "\n",
    "    load_dotenv()\n",
    "    host = os.environ.get('DB_DESTINATION_HOST')\n",
    "    port = os.environ.get('DB_DESTINATION_PORT')\n",
    "    db = os.environ.get('DB_DESTINATION_NAME')\n",
    "    username = os.environ.get('DB_DESTINATION_USER')\n",
    "    password = os.environ.get('DB_DESTINATION_PASSWORD')\n",
    "    \n",
    "    print(f'postgresql://{username}:{password}@{host}:{port}/{db}')\n",
    "    conn = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{db}', connect_args={'sslmode':'require'})\n",
    "    return conn\n",
    "\n",
    "def get_data():\n",
    "    with open('params.yaml', 'r') as fd:\n",
    "        params = yaml.safe_load(fd)\n",
    "\n",
    "    conn = create_connection()\n",
    "    data = pd.read_sql('select * from clean_users_churn', conn, index_col=params['index_col'])\n",
    "    conn.dispose()\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    data.to_csv('data/initial_data.csv', index=None)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные разделены\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def fit_model():\n",
    "    with open('params.yaml', 'r') as fd:\n",
    "        params = yaml.safe_load(fd)\n",
    "    \n",
    "    data = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "    X = data.drop(columns=[params['target_col'], 'end_date']) # Признаки без утечек\n",
    "    y = data[params['target_col']] # Целевая переменная\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    print(\"Данные разделены\")\n",
    "\n",
    "    cat_features = X.select_dtypes(include='object')\n",
    "    potential_binary_features = cat_features.nunique() == 2\n",
    "\n",
    "    binary_cat_features = cat_features[potential_binary_features[potential_binary_features].index]\n",
    "    other_cat_features = cat_features[potential_binary_features[~potential_binary_features].index]\n",
    "    num_features = X.select_dtypes(['float'])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "        ('binary', OneHotEncoder(drop=params['one_hot_drop']), binary_cat_features.columns.tolist()),\n",
    "        ('cat', OneHotEncoder(drop=params['one_hot_drop']), other_cat_features.columns.tolist()),\n",
    "        ('num', StandardScaler(), num_features.columns.tolist())\n",
    "        ],\n",
    "        remainder='drop',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    model = LogisticRegression(C=params['C'], penalty=params['penalty'], max_iter=200)\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ]\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train) \n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open('models/fitted_model.pkl', 'wb') as fd:\n",
    "        joblib.dump(pipeline, fd)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tfit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные разделены\n",
      "{'err1': 0.0731244064577398, 'err2': 0.1419753086419753, 'auc': 0.8590278232211164, 'precision': 0.6600441501103753, 'recall': 0.5588785046728972, 'f1': 0.605263157894737, 'logloss': 0.3947864292386493}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import yaml\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, log_loss\n",
    "\n",
    "with open('params.yaml', 'r') as fd:\n",
    "    params = yaml.safe_load(fd)\n",
    "\n",
    "with open('models/fitted_model.pkl', 'rb') as fd:\n",
    "    model = joblib.load(fd)\n",
    "\n",
    "data = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "X = data.drop(columns=[params['target_col'], 'end_date']) # Признаки без утечек\n",
    "y = data[params['target_col']] # Целевая переменная\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(\"Данные разделены\")\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "probas = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Заводим словарь для хранения метрик\n",
    "metrics = {}\n",
    "\n",
    "# Подсчитываем матрицу ошибок (конфузионную матрицу)\n",
    "_, err1, _, err2 = confusion_matrix(y_test, prediction, normalize='all').ravel()\n",
    "\n",
    "# Подсчитываем метрики\n",
    "auc = roc_auc_score(y_test, probas)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "logloss = log_loss(y_test, probas)\n",
    "\n",
    "# Записываем значения метрик в словарь\n",
    "metrics[\"err1\"] = err1\n",
    "metrics[\"err2\"] = err2\n",
    "metrics[\"auc\"] = auc\n",
    "metrics[\"precision\"] = precision\n",
    "metrics[\"recall\"] = recall\n",
    "metrics[\"f1\"] = f1\n",
    "metrics[\"logloss\"] = logloss\n",
    "\n",
    "# Выводим метрики\n",
    "print(metrics)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "        metrics[key] = round(value.mean(), 3) \n",
    "\n",
    "os.makedirs('cv_results', exist_ok=True)\n",
    "with open('cv_results/cv_res.json', 'w') as fd:\n",
    "    json.dump(metrics, fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-mlflow/.venv_mle_mlflow/lib/python3.10/site-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
      "Registered model 'churn_model_krosh' already exists. Creating a new version of this model...\n",
      "2024/07/06 13:29:06 INFO mlflow.tracking._model_registry.client: Waiting up to 60 seconds for model version to finish creation. Model name: churn_model_krosh, version 7\n",
      "Created version '7' of model 'churn_model_krosh'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "EXPERIMENT_NAME = \"krosh_exp\"\n",
    "RUN_NAME = \"model_0_registry\"\n",
    "REGISTRY_MODEL_NAME = \"churn_model_krosh\"\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "pip_requirements = \"../requirements.txt\"\n",
    "signature = mlflow.models.infer_signature(X_test, prediction)\n",
    "input_example = X_test[:2]\n",
    "metadata = {'model_type': 'monthly'}\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    metrics = {\n",
    "        \"err1\": err1,\n",
    "        \"err2\": err2,\n",
    "        \"auc\": auc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"logloss\": logloss\n",
    "    }\n",
    "    \n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "    sk_model=model,\n",
    "    await_registration_for=60,\n",
    "    signature=signature,\n",
    "    input_example=input_example,\n",
    "    metadata=metadata,\n",
    "    pip_requirements=pip_requirements,\n",
    "    registered_model_name=REGISTRY_MODEL_NAME,\n",
    "    artifact_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3a3f4c811d48e2bf20610830ccc602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.sklearn.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "model_predictions = loaded_model.predict(X_test)\n",
    "\n",
    "assert model_predictions.dtype == int\n",
    "\n",
    "print(model_predictions[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mle_mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
